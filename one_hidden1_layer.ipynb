{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "one_hidden1_layer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "coursera": {
      "course_slug": "neural-networks-deep-learning",
      "graded_item_id": "wRuwL",
      "launcher_item_id": "NI888"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2wpC_YO9WDZ3"
      },
      "source": [
        "# Classification with one hidden layer\n",
        "\n",
        "Welcome to this case study of implementing a **2-Layer Neural Network(1-hidden layer) with numpy package.**\n",
        "\n",
        "--------------------------------------------------------------------------------\n",
        "You will do the following tasks in this case study.\n",
        "\n",
        "\n",
        "- Implement a Binary classification neural network with a single hidden layer Neural Network.\n",
        "- Implement the forward progation and backward propagation of network.\n",
        "- Implement the non-linear activation functions\n",
        "\n",
        "\n",
        "------------------------------------------------\n",
        "\n",
        "\n",
        "**Note:**: Don't delete the instructions or any of the cells\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsxdGqE2SAj4",
        "colab_type": "text"
      },
      "source": [
        "### 1. Import the Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F1H13meTZKAp",
        "colab": {}
      },
      "source": [
        "## Import the packages\n",
        "\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import sklearn.datasets\n",
        "import sklearn.linear_model\n",
        "#from IPython.display import display, Math, Latex\n",
        "\n",
        "np.random.seed(1) # set a seed so that the results are consistent\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IJ0xgE9aWDaT"
      },
      "source": [
        "### 2. Loading and reshaping the Dataset ##\n",
        "\n",
        "First, let's get the dataset you will work on. The following code will load a \"Moon\" 2-class dataset into variables `X` and `Y`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5k1n21vAWDcC",
        "colab": {}
      },
      "source": [
        "#Load the Dataset\n",
        "noisy_moons = sklearn.datasets.make_moons(n_samples=400, noise=.2)\n",
        "X, Y = noisy_moons\n",
        "\n",
        "\"\"\"\n",
        "Reshape the dataset such that \n",
        "X has shape (2,400)\n",
        "Y has shape (1,200)\n",
        "\"\"\"\n",
        "#START YOUR CODE HERE\n",
        "X=\n",
        "Y=\n",
        "#END YOUR CODE HERE\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1j49CHf0WDeT"
      },
      "source": [
        "### 3. Neural Network model\n",
        "\n",
        "\n",
        "\n",
        "The general methodology to build a Neural Network is to:\n",
        "\n",
        "    1. Define the neural network structure ( # of input units,  # of hidden units, etc). \n",
        "    2. Initialize the model's parameters\n",
        "    3. Loop:\n",
        "        - Implement forward propagation\n",
        "        - Compute loss\n",
        "        - Implement backward propagation to get the gradients\n",
        "        - Update parameters (gradient descent)\n",
        "\n",
        "You often build helper functions to compute steps 1-3 and then merge them into one function we call `nn_model()`. Once you've built `nn_model()` and learnt the right parameters, you can make predictions on new data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_LXyov7hWDeW"
      },
      "source": [
        "### 3. Defining the neural network structure ####\n",
        "\n",
        "\n",
        "This function will help you in defining the following three variables:\n",
        "\n",
        "\n",
        "- **n_x**: The size of the input layer (No. of neurons in Input layer representing the features of a single training example)\n",
        "- **n_h**: The size of the hidden layer (No. of neurons in hidden layer)\n",
        "- **n_y**: The size of the output layer(No. of Neurons representing the output classes)\n",
        "\n",
        "\n",
        "Note: Refer the picture in question for more details\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9tCBQjUvWDea",
        "colab": {}
      },
      "source": [
        "def layer_sizes(X, Y):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "    n_x -- the size of the input layer\n",
        "    n_h -- the size of the hidden layer\n",
        "    n_y -- the size of the output layer\n",
        "    \"\"\"\n",
        "    # START CODE \n",
        "    n_x =\n",
        "    n_y = \n",
        "    # END YOUR CODE\n",
        "    n_h = 4\n",
        "   \n",
        "    return (n_x, n_h, n_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q4Dk3aNVWDex"
      },
      "source": [
        "### 4. Initialize the weights and biases for the layers (hidden layer and output layer)####\n",
        "\n",
        "In this function you will initialise the weights and biases for hidden layer and output layer.\n",
        "\n",
        "```\n",
        "W1,b1 - Weight and Bias of Hidden Layer\n",
        "W2,b2 - weight and Bias of Output Layer\n",
        "```\n",
        "NOTE:\n",
        "\n",
        "1. Weights has to be initialised with random values.`np.random.randn(rows,columns) * 0.01`\n",
        "2. Biases has to be initialised with  numpy Zeros `np.zeros((rows,columns))`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8aJTlNhoWDez",
        "colab": {}
      },
      "source": [
        "\n",
        "def initialize_parameters(n_x, n_h, n_y):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "    Layer sizes\n",
        "\n",
        "    Output:\n",
        "    parameters -- dictionary containing W1,b1,W2,b2\n",
        "                    \n",
        "    \"\"\"\n",
        "    \n",
        "    #Please don't remove this line of code\n",
        "    np.random.seed(2) \n",
        "    \n",
        "    \n",
        "\n",
        "    #Initialise the parameters\n",
        "    '''\n",
        "    Hidden Layer \n",
        "    W1 -- weight matrix of shape (n_h, n_x)\n",
        "    b1 -- bias vector of shape (n_h, 1)\n",
        "\n",
        "    Output Layer\n",
        "    W2 -- weight matrix of shape (n_y, n_h)\n",
        "    b2 -- bias vector of shape (n_y, 1)\n",
        "    ''''\n",
        "    #START CODE HERE\n",
        "    W1 = \n",
        "    b1 = \n",
        "    W2 = \n",
        "    b2 = \n",
        "    #END CODE HERE\n",
        "    \n",
        "    #Adding the parameters to dictionary    \n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-mHIguzd7jA",
        "colab_type": "text"
      },
      "source": [
        "### 5. Define the Non-Linear Activation Function \n",
        "```\n",
        "- Activation Function for Hidden layer : Sigmoid\n",
        "- Activation Function for Output layer : tanh (Numpy package: `np.tanh())\n",
        "```\n",
        "Let's define the sigmoid function\n",
        "\n",
        "Given an input `x`, `sigmoid(x)` can be calulated with the formula $\\frac{1}{1+e^{-x}}$\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsKO0qTufK17",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Sigmoid activation function\n",
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "    x -- An array of any size.\n",
        "    Return:\n",
        "    y -- sigmoid(x)\n",
        "    \"\"\"\n",
        "\n",
        "    #START YOUR CODE\n",
        "\n",
        "    y = \n",
        "\n",
        "    #END YOUR CODE\n",
        "\n",
        "\n",
        "    return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6reMJ3Ejc3X",
        "colab_type": "text"
      },
      "source": [
        "### 6. Implement the Forward Propagation\n",
        "\n",
        "In this function you will have to compute the values of \n",
        "\n",
        "$Z^{[1]},A^{[1]},Z^{[2]},A^{[2]}$\n",
        "\n",
        "and return \n",
        "\n",
        "- The  activation ouput of ouput layer A2\n",
        "- A dictionary which stores the values of Z1,A1,Z2,A2\n",
        "\n",
        "\n",
        "\n",
        "**Formulae:**\n",
        "\n",
        "1. $Z^{[1]}=W^{[1]}*X+b^{[1]}$\n",
        "2. $A^{[1]}=tan(Z^{[1]})$\n",
        "\n",
        "3. $Z^{[2]}=W^{[2]}*A^{[1]}+b^{[2]}$\n",
        "4. $A^{[2]}=sigmoid(Z^{[2]})$\n",
        "\n",
        "\n",
        "*Note : The superscript value denotes the layer number*\n",
        "\n",
        "*Hint : Use the function np.dot() for multiply the matrices*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7-wpDtbwWDfM",
        "colab": {}
      },
      "source": [
        "#Implement the Forward Propagation\n",
        "\n",
        "def forward_propagation(X, parameters):\n",
        "    \"\"\"\n",
        "    Input \n",
        "    X : Input of shape(2,400)\n",
        "    parameters -- Dictionary containing the weights and biases\n",
        "    \n",
        "    Output:\n",
        "    A2 -- The sigmoid output of the second activation\n",
        "    cache -- a dictionary containing the values of \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
        "    \"\"\"\n",
        "    # Retrieve the weights and biases from the dictionary parameters\n",
        "    # START CODE HERE \n",
        "    W1 = \n",
        "    b1 = \n",
        "    W2 = \n",
        "    b2 = \n",
        "    #END CODE HERE\n",
        "    \n",
        "    # Calculate the following \n",
        "    #Hint : Use the function np.dot() for multiply the matrices\n",
        "    # START CODE HERE\n",
        "    Z1 = \n",
        "    A1 = \n",
        "    Z2 = \n",
        "    A2 = \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    \n",
        "    cache = {\"Z1\": Z1,\n",
        "             \"A1\": A1,\n",
        "             \"Z2\": Z2,\n",
        "             \"A2\": A2}\n",
        "    \n",
        "    return A2, cache\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mkOGKTYJWDfk"
      },
      "source": [
        "### 7. Compute the cost\n",
        "\n",
        "In this function you will compute the cost with the formula\n",
        " $$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large{(} \\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right) \\large{)} \\small$$\n",
        "\n",
        " Here \n",
        " \n",
        " - $a^{[2](i)}$ denotes the predicted output of training example `i`\n",
        " - $y^{(i)}$ denotes the true labels of training example `i`\n",
        " - $m$ denotes the total number of training examples\n",
        "\n",
        "\n",
        "**Note:** Here `i` denotes the number of training example\n",
        "\n",
        "**Hint**:\n",
        "Use np.sum() and np.multiply()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nbOJVe8YWDfn",
        "colab": {}
      },
      "source": [
        "# compute_cost\n",
        "\n",
        "def compute_cost(A2, Y, parameters):\n",
        "    \"\"\"\n",
        "    \n",
        "    Input:\n",
        "    A2 : The sigmoid output of the second activation (Predicted labels)\n",
        "    Y : True labels\n",
        "    parameters -- Dictionary containing weights and biases W1, b1, W2 and b2\n",
        "    \n",
        "    Returns:\n",
        "    cost \n",
        "    \"\"\"\n",
        "    \n",
        "    #-----------------Assign the total number of examples to variable \"m\"--------------\n",
        "    #START YOUR CODE\n",
        "    m = \n",
        "\n",
        "    #END YOUR CODE\n",
        "\n",
        "\n",
        "    \n",
        "       \n",
        "    #-------------------Compute the cost-------------------------------\n",
        "    #START YOUR CODE\n",
        "    cost = \n",
        "    #END CODE HERE\n",
        "    \n",
        "    #--------------make sure to check the Datatype-------------  \n",
        "    cost = np.squeeze(cost)            \n",
        "    assert(isinstance(cost, float))\n",
        "    \n",
        "    return cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "U7ttFIV4nrxI"
      },
      "source": [
        "### 8. Implement the Backward Propagation\n",
        "\n",
        "In this function you will be finding the derivatives (gradients) to update the parameters\n",
        "\n",
        "\n",
        "\n",
        "Formulae:\n",
        "\n",
        "$dZ^{[2]}=A^{[2]}-Y$\n",
        "\n",
        "$dW^{[2]}=\\frac{1}{m}dZ^{[2]}A^{[1]T}$\n",
        "\n",
        "$db^{[2]} = \\frac{1}{m}np.sum(dZ^{[2]}, axis=1, keepdims=True)$\n",
        "\n",
        "$dz^{[1]}=W^{[2]T}dZ^{[2]}* (1-(A^{[1]})^{2})$\n",
        "\n",
        "\n",
        "$dW^{[1]}=\\frac{1}{m}dZ^{[1]}X^{T}$\n",
        "\n",
        "$db^{[1]} = \\frac{1}{m}np.sum(dZ^{[1]}, axis=1, keepdims=True)$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hq-77v0pnrDf",
        "colab": {}
      },
      "source": [
        "# GRADED FUNCTION: backward_propagation\n",
        "\n",
        "def backward_propagation(parameters, cache, X, Y):\n",
        "    \"\"\"\n",
        "       \n",
        "    Input:\n",
        "    parameters - Dictionary containing weights and biases\n",
        "    cache - Dictionary containing Z1,A1,Z2,A2\n",
        "    X -- Input data \n",
        "    Y -- true labels \n",
        "    \n",
        "    Returns:\n",
        "    grads -- python dictionary containing your gradients with respect to different parameters\n",
        "    \"\"\"\n",
        "    #----------Assign the number of training examples------------\n",
        "    m = \n",
        "\n",
        "\n",
        "    \n",
        "    #-------Retrieve W1 and W2--------------------\"\n",
        "    #START YOUR CODE\n",
        "    W1 =\n",
        "    W2 =\n",
        "    #END YOUR CODE\n",
        "\n",
        "        \n",
        "    # ------------Retrieve A1 and A2 --------------.\n",
        "    #START YOUR CODE\n",
        "    A1 = \n",
        "    A2 = \n",
        "    #END YOUR CODE\n",
        "    \n",
        "    # ----------calculate dW1, db1, dW2, db2-----------------\n",
        "    #START CODE HERE \n",
        "    dZ2= \n",
        "    dW2 = \n",
        "    db2 = \n",
        "    dZ1 = \n",
        "    dW1 = \n",
        "    db1 = \n",
        "    #END CODE HERE\n",
        "    \n",
        "    grads = {\"dW1\": dW1,\n",
        "             \"db1\": db1,\n",
        "             \"dW2\": dW2,\n",
        "             \"db2\": db2}\n",
        "    \n",
        "    return grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ary0LUNlnmkG"
      },
      "source": [
        "### 9. Update the weights and biases\n",
        "\n",
        "\n",
        "**Update rule:**\n",
        "\n",
        "If W is the parameter to be updated then\n",
        "\n",
        "$W=W-\\alpha*dW$\n",
        "\n",
        "where\n",
        "\n",
        "- $\\alpha$ is the learning rate\n",
        "\n",
        "- $dW$ is the derivative w.r to cost J (i.e)$\\frac{dJ}{dW}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3xfZ44AynmAy",
        "colab": {}
      },
      "source": [
        "\n",
        "def update_parameters(parameters, grads, learning_rate=1.2):\n",
        "    \"\"\"\n",
        "  \n",
        "    \n",
        "    Input:\n",
        "    parameters : python dictionary containing your parameters \n",
        "    grads: python dictionary containing your gradients \n",
        "    \n",
        "    Output:\n",
        "    parameters: dictionary of updated parameters\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    # Update \n",
        "    ### START CODE HERE ### (â‰ˆ 4 lines of code)\n",
        "    W1 = \n",
        "    b1 = \n",
        "    W2 = \n",
        "    b2 = \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "At7XOAL2WDg_"
      },
      "source": [
        "### 10. Combine them all in right order\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "trSIgL5nWDhB",
        "colab": {}
      },
      "source": [
        "# GRADED FUNCTION: nn_model\n",
        "\n",
        "def nn_model(X, Y, n_h, num_iterations=10000, print_cost=False):\n",
        "    \"\"\"\n",
        "        \n",
        "    Returns:\n",
        "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
        "    \"\"\"\n",
        "    \n",
        "    np.random.seed(3)\n",
        "\n",
        "    #-----------------Call \"layer_sizes\" function to assign the values----------------------\n",
        "    n_x = \n",
        "    n_y =\n",
        "    \n",
        "    # ----------------Call \"initialize_parameters\" functions to assign the weights and biases---------\n",
        "    #START CODE\n",
        "    \n",
        "    W1 = \n",
        "    b1 = \n",
        "    W2 = \n",
        "    b2 = \n",
        "    #END CODE\n",
        "    \n",
        "\n",
        "    for i in range(0, num_iterations):\n",
        "         \n",
        "        #---------------START CODE HERE ----------------------\n",
        "\n",
        "\n",
        "        # -----------Call forward_propagation function-------\n",
        "        A2, cache = \n",
        "        \n",
        "        # ------------Call compute_cost function----------\n",
        "        cost = \n",
        " \n",
        "        # -----------Call backward_propagation function-----------\n",
        "        grads = \n",
        " \n",
        "        # --------Call update_parameters function---------\n",
        "        parameters =\n",
        "        \n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        # Print the cost every 500 iterations\n",
        "        if print_cost and i % 500 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" % (i, cost))\n",
        "\n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u97Yq4NAWDhW"
      },
      "source": [
        "### Run the below cells to check the accuracy of predictions made\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JuYSpvFaWDhZ",
        "colab": {}
      },
      "source": [
        "\n",
        "def predict(parameters, X):\n",
        "       \n",
        "    \n",
        "    ### START CODE HERE \n",
        "    A2, cache = forward_propagation(X, parameters)\n",
        "    predictions = np.round(A2)\n",
        "    ### END CODE HERE \n",
        "    \n",
        "    return predictions\n",
        "\n",
        "\n",
        "parameters = nn_model(X, Y, n_h = 4, num_iterations=10000, print_cost=True)\n",
        "predictions = predict(parameters, X)\n",
        "print ('Accuracy: %d' % float((np.dot(Y, predictions.T) + np.dot(1 - Y, 1 - predictions.T)) / float(Y.size) * 100) + '%')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hn3RFMVkMqo5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INcmTBF1MqpA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}